\documentclass[12pt]{article}

% Packages
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage[utf8]{inputenc}
\usepackage{natbib}
\usepackage{lipsum} % for generating filler text

\title{RAG-based Personalization and LLMs Evaluation for AI Chatbots}
\author{Enrico Zanetti}
\date{%
    University of Trento\\[2ex]%
    \today
}

\begin{document}

\maketitle

\begin{abstract}
This thesis analyzes the evolving landscape of chatbot personalization and evaluation, focusing on the main technology: Large Language Models (LLMs). The study begins with an introductory overview of the existing fields of application of chatbots, within the realms of healthcare, education, customer support, finance, and business, and presents an overview of chatbot personalization. It delves into the Transformer architecture’s core elements and examines LLM training processes, including pre-training and fine-tuning, as well as innovative prompt-based learning methods. The study highlights the enhancement of information retrieval through the Retrieval Augmented Generation (RAG) Framework and presents state-of-the-art models like GPT-4 and Claude 3 comparing their performance, limitations and challenges.
\newline
Moreover, the study addresses the transition from rule-based to LLM-based chatbots and the critical role of system evaluation, introducing a case study on WISE, a proprietary chatbot of HPA based on LLM that leverages RAG technology to retrieve documents information. As part of my internship project, a LLM Evaluator implemented for WISE is introduced, illustrating the workflow, metrics, methodologies, and proposed future enhancements. Additionally, the thesis delves into the ethical and regulatory framework in AI. Finally, key findings are summarized and are presented suggesting directions for future research in chatbot technology.
\end{abstract}

\section*{Disclaimer}
\textit{The current index, title, and list of references are temporary and subject to modification; they introduce the general topic and main scope of the research. This document will be updated with respect to the changes during the work.}

\tableofcontents
\newpage

\section{Introduction}
\subsection{Introduction}

\subsection{Related Work}
Review of related work focusing on LLM-based chatbots in healthcare, education, customer-support, finance, business.

\subsection{Chatbot personalization}
Evolution of chatbot personalization. Previous methods: Example-Based Dialogue Management (EBDM) and ML-based approaches for personality prediction.

\section{Methodology}
\subsection{Large Language Models (LLMs)}
Introduction and brief history of LLMs.

\subsection{The Transformer architecture}
Description of the Transformer architecture including its core elements: tokenization, positional encoding, self-attention mechanism, Multi-head attention, feed-forward Neural Networks, Add and norm step, Residual connection (add), Layer normalization (norm), Encoder and decoder stacks, Final linear and softmax layer.

\subsection{Model Size}
Importance of model size and review of parameter size of AI Neural Networks over the past decades.

\subsection{Training Process}
Explanation of the training process, divided into three key parts: Pre-training, Fine-tuning, and Prompt-based Learning.

\subsubsection{Pre-training}
Pre-training methods in transformer-based LLMs.

\subsubsection{Next-token and Multi-token prediction}
Note: start reading @gloeckle2024better \newline
Overview of next-token prediction and a novel methodology to train LLMs: multi-token prediction, an improvement over next-token prediction in training language models for generative or reasoning tasks. 

\subsection{Fine-tuning}
Scope and techniques.

\subsubsection{LoRA: Low-Rank Adaptation}
The Low-Rank Adaptation (LoRA) technique reduces the number of trainable parameters by using a matrix rank decomposition.

\subsubsection{Prompt-based Learning}
Zero-Shot and Few-Shot Learning, Chain-of-Thought Reasoning

\subsection{Augmented LLMs}
Review of recent literature on incorporating external knowledge into task-oriented dialogue systems.

\subsubsection{Retrieval Augmented Generation Framework}
Description of the retriever, generator, training, and decoding.

\subsection{Evaluating Large Language Model (LLM) systems}
Assessing applications powered by Large Language Models (LLMs) holds pivotal importance in our technological landscape. These evaluations not only gauge performance but also address the myriad of challenges we endeavor to overcome (bias, overfitting, hallucination, attribution, staleness, revisions, accuracy on retrieving information).

\subsubsection{Illustrations and Approaches}

From the innovative OpenAI Evals platform to pioneering concepts like LLM-as-a-Judge, the spectrum of evaluation techniques continues to evolve. Additionally, emerging methodologies such as adversarial testing and bias analysis contribute to a comprehensive understanding of LLM capabilities and limitations.

\subsection{State-of-the-art Models}
Selection of state-of-the-art LLMs (GPT-4o, Claude 3.5, Gemini Pro, Mistral, Meta Llama 3,...) to compare their differences in performance, architecture, accessibility and cost for developing question-answering chatbot systems.

\subsubsection{Challenges and limitations}
Discussion on cost, carbon footprint and energy consumption and privacy concerns.

\subsubsection{Beyond Transformers: the Mamba LLM Architecture}
Discover the power of Mamba LLM, a transformative architecture from leading universities, redefining sequence processing in AI.

\section{Experiments}
\subsection{WISE: a Case Study}
\subsubsection{Introduction}


Overview of WISE, implementation, architecture, practical applications, position in the market.

\subsubsection{Architecture}
Description of the architecture, including the components involved in its operation, such as the LLM model, retrieval mechanism, natural language understanding (NLU), natural language generation (NLG), and any other relevant modules.

\subsubsection{Implementation}
Discussion of the technical implementation of WISE, including the technologies and frameworks used, data preprocessing steps, model training procedures, and integration with external systems or databases.

\subsubsection{Case Studies}
Real-world examples demonstrating the chatbot’s utility and effectiveness in solving users’ problems.

\subsubsection{Future Enhancements}
Discussion of potential areas for improvement or future development of the chatbot, including new features, optimizations, or expansions into additional domains or languages.

\subsection{HPA LLM Evaluator}
Presentation of my internship project: the LLM Evaluator for WISE. Workflow, metrics and methodologies, limitations and future improvements.

\subsubsection{Metrics and Methodologies}
Description of metrics and methodologies used within the LLM Evaluator: Similarity Score, RAGAS, Reasoning (via prompt engineering), Subjectivity.

\subsubsection{Tests and results}
Overview of personalized prompts used for the evaluator and the experiments with different metrics. Visualization of the results.

\subsubsection{Limitations}
Discussion on limitations of the LLM evaluator.

\subsubsection{Future Work}
Further improvements and exploration.

\section{Ethics and Regulations}
Ethical and societal considerations in AI development, challenges in regulating AI, global and continental collaboration on AI regulation: focus on the AI Act.

\section{Conclusion}
The thesis concludes by summarizing the key findings.

\newpage

\nocite{*}
\bibliographystyle{plain}
\bibliography{mybibliography}

\end{document}
