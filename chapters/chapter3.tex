\section{Large Language Models (LLMs)}
Introduction and brief history of LLMs.

\section{The Transformer architecture}
Description of the Transformer architecture including its core elements: tokenization, positional encoding, self-attention mechanism, Multi-head attention, feed-forward Neural Networks, Add and norm step, Residual connection (add), Layer normalization (norm), Encoder and decoder stacks, Final linear and softmax layer.

\section{Model Size}
Importance of model size and review of parameter size of AI Neural Networks over the past decades.

\section{Training Process}
Explanation of the training process, divided into three key parts: Pre-training, Fine-tuning, and Prompt-based Learning.

\subsection{Pre-training}
Pre-training methods in transformer-based LLMs.

\subsection{Next-token and Multi-token prediction}
Note: start reading @gloeckle2024better \newline
Overview of next-token prediction and a novel methodology to train LLMs: multi-token prediction, an improvement over next-token prediction in training language models for generative or reasoning tasks. 

\section{Fine-tuning}
Scope and techniques.

\subsection{LoRA: Low-Rank Adaptation}
The Low-Rank Adaptation (LoRA) technique reduces the number of trainable parameters by using a matrix rank decomposition.

\subsection{Prompt-based Learning}
Zero-Shot and Few-Shot Learning, Chain-of-Thought Reasoning

\section{Augmented LLMs}
Review of recent literature on incorporating external knowledge into task-oriented dialogue systems.

\subsection{Retrieval Augmented Generation Framework}
Description of the retriever, generator, training, and decoding.

\section{Evaluating and Tracking Large Language Models}
Assessing applications powered by Large Language Models (LLMs) holds pivotal importance in our technological landscape. These evaluations not only gauge performance but also address the myriad of challenges we endeavor to overcome (bias, overfitting, hallucination, attribution, staleness, revisions, accuracy on retrieving information).

\subsection{Illustrations and Approaches}

From the innovative OpenAI Evals platform to pioneering concepts like LLM-as-a-Judge, the spectrum of evaluation techniques continues to evolve. Additionally, emerging methodologies such as adversarial testing and bias analysis contribute to a comprehensive understanding of LLM capabilities and limitations.

\subsection{The Importance of ML Experiment Tracking}

\subsection{Introduction to MLtraq: An Open-source Python Library}

\section{State-of-the-art Models}
Selection of state-of-the-art LLMs (GPT-4o, Claude 3.5, Gemini Pro, Mistral, Meta Llama 3,...) to compare their differences in performance, architecture, accessibility and cost for developing question-answering chatbot systems.

\subsection{Challenges and Limitations}
Discussion on cost, carbon footprint and energy consumption and privacy concerns.

\subsection{Beyond Transformers: the Mamba LLM Architecture}
Discover the power of Mamba LLM, a transformative architecture from leading universities, redefining sequence processing in AI.

