@article{adamopoulou2020chatbots,
  title={Chatbots: History, technology, and applications},
  author={Adamopoulou, Eleni and Moussiades, Lefteris},
  journal={Machine Learning with Applications},
  volume={2},
  year={2020}
}

@article{gu2023mamba,
  title={Mamba: Linear-time sequence modeling with selective state spaces},
  author={Gu, Albert and Dao, Tri},
  journal={arXiv preprint arXiv:2312.00752},
  year={2023}
}

@article{gloeckle2024better,
  title={Better \& Faster Large Language Models via Multi-token Prediction},
  author={Gloeckle, Fabian and Idrissi, Badr Youbi and Rozi{\`e}re, Baptiste and Lopez-Paz, David and Synnaeve, Gabriel},
  journal={arXiv preprint arXiv:2404.19737},
  year={2024}
}

@article{hu2021lora,
  title={Lora: Low-rank adaptation of large language models},
  author={Hu, Edward J and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu},
  journal={arXiv preprint arXiv:2106.09685},
  year={2021}
}

@inproceedings{bang2015example,
  title={Example-based chat-oriented dialogue system with personalized long-term memory},
  author={Bang, Jeesoo and Noh, Hyungjong and Kim, Yonghee and Lee, Gary Geunbae},
  booktitle={2015 International Conference on Big Data and Smart Computing (BIGCOMP)},
  pages={238--243},
  organization={IEEE},
  year={2015}
}

@misc{hendrycks2020measuring,
  title={Measuring massive multitask language understanding},
  author={Hendrycks, Dan and Burns, Collin and Basart, Steven and Zou, Andy and Mazeika, Mantas and Song, Dawn and Steinhardt, Jacob},
  year={2020},
  note={arXiv preprint arXiv:2009.03300}
}

@inproceedings{kim2015acquisition,
  title={Acquisition and use of long-term memory for personalized dialog systems},
  author={Kim, Yonghee and Bang, Jeesoo and Choi, Junhwi and Ryu, Seonghan and Koo, Sangjun and Lee, Gary Geunbae},
  booktitle={Multimodal Analyses enabling Artificial Agents in Human-Machine Interaction},
  volume={2},
  pages={78--87},
  organization={Springer International Publishing},
  year={2015}
}

@misc{lin2021truthfulqa,
  title={TruthfulQA: Measuring how models mimic human falsehoods},
  author={Lin, Stephanie and Hilton, Jacob and Evans, Owain},
  year={2021},
  note={arXiv preprint arXiv:2109.07958}
}

@article{mattas2023chatgpt,
  title={ChatGPT: A study of AI language processing and its implications},
  author={Mattas, Puranjay Savar},
  journal={Journal homepage: www.ijrpr.com},
  volume={2582},
  number={7421},
  year={2023}
}

@misc{nature2023powerful,
  title={Powerful AI models, and more — this week’s best science graphics},
  author={Nature},
  year={2023},
  note={\url{https://www.nature.com/articles/d41586-023-00777-9}}
}

@misc{wandb2023,
  title={Intro to MLOps: Machine Learning Experiment Tracking},
  author={Weights \& Biases},
  year={2023},
  note={\url{https://wandb.ai/site/articles/intro-to-mlops-machine-learning-experiment-tracking}}
}

@misc{mltraq2024,
  title={MLtraq: Machine Learning Experiment Tracking},
  author={Michele Dalla Chiesa},
  year={2024},
  note={\url{https://mltraq.com/}}
}

@misc{sqlite2024,
  title={Faster Than The Filesystem},
  author={{SQLite}},
  year={2024},
  note={\url{https://www.sqlite.org/fasterthanfs.html}}
}

@article{weizenbaum1966eliza,
  title={ELIZA—a computer program for the study of natural language communication between man and machine},
  author={Weizenbaum, Joseph},
  journal={Communications of the ACM},
  volume={9},
  number={1},
  pages={36--45},
  year={1966},
  publisher={ACM New York, NY, USA}
}

@article{winograd1972understanding,
  title={Understanding natural language},
  author={Winograd, Terry},
  journal={Cognitive psychology},
  volume={3},
  number={1},
  pages={1--191},
  year={1972},
  publisher={Elsevier}
}

@misc{researchgraph2024,
  author = {ResearchGraph},
  title = {Brief Introduction to the History of Large Language Models (LLMs)},
  year = {2024},
  howpublished = {\url{https://medium.com/@researchgraph/brief-introduction-to-the-history-of-large-language-models-llms-3c2efa517112}}
}

@misc{AnalyticsVidhya2023,
  author = {AnalyticsVidhya},
  title = {Brief History of Large Language Models & Generative AI | Evolution of NLP from Eliza to ChatGPT},
  year = {2023},
  note = {Accessed: 2024-07-27},
  howpublished = {\url{https://www.youtube.com/watch?v=K7o5_Fj7_SY}}
}

@article{elman1990finding,
  title={Finding structure in time},
  author={Elman, Jeffrey L},
  journal={Cognitive science},
  volume={14},
  number={2},
  pages={179--211},
  year={1990},
  publisher={Wiley Online Library}
}

@article{hochreiter1997long,
  title={Long short-term memory},
  author={Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
  journal={Neural computation},
  volume={9},
  number={8},
  pages={1735--1780},
  year={1997},
  publisher={MIT press}
}

@article{cho2014learning,
  title={Learning phrase representations using RNN encoder-decoder for statistical machine translation},
  author={Cho, Kyunghyun and Van Merri{\"e}nboer, Bart and Gulcehre, Caglar and Bahdanau, Dzmitry and Bougares, Fethi and Schwenk, Holger and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1406.1078},
  year={2014}
}

@article{bahdanau2014neural,
  title={Neural machine translation by jointly learning to align and translate},
  author={Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1409.0473},
  year={2014}
}

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@article{devlin2018bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}

@article{radford2019language,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya and others},
  journal={OpenAI blog},
  volume={1},
  number={8},
  pages={9},
  year={2019}
}

@misc{geeksforgeeks2024-pe,
  title={Positional Encoding in Transformers},
  author={GeeksforGeeks},
  howpublished={\url{https://www.geeksforgeeks.org/positional-encoding-in-transformers/}},
  year={2024},
  note={Accessed: 2024-07-28}
}

@misc{geeksforgeeks2024-sa,
  title={Self-Attention in NLP},
  author={{GeeksforGeeks}},
  year={2023},
  howpublished={\url{https://www.geeksforgeeks.org/self-attention-in-nlp/}},
  note={Accessed: 2024-07-28}
}

@article{pires2023one,
  title={One wide feedforward is all you need},
  author={Pires, Telmo Pessoa and Lopes, Ant{\'o}nio V and Assogba, Yannick and Setiawan, Hendra},
  journal={arXiv preprint arXiv:2309.01826},
  year={2023}
}

@article{li2023transformer,
  title={Transformer for object detection: Review and benchmark},
  author={Li, Yong and Miao, Naipeng and Ma, Liangdi and Shuang, Feng and Huang, Xingwen},
  journal={Engineering Applications of Artificial Intelligence},
  volume={126},
  pages={107021},
  year={2023},
  publisher={Elsevier}
}

@article{kazemnejad2019:pencoding,
  title   = "Transformer Architecture: The Positional Encoding",
  author  = "Kazemnejad, Amirhossein",
  journal = "kazemnejad.com",
  year    = "2019",
  url     = "https://kazemnejad.com/blog/transformer_architecture_positional_encoding/"
}

@article{chowdhery2023palm,
  title={Palm: Scaling language modeling with pathways},
  author={Chowdhery, Aakanksha and Narang, Sharan and Devlin, Jacob and Bosma, Maarten and Mishra, Gaurav and Roberts, Adam and Barham, Paul and Chung, Hyung Won and Sutton, Charles and Gehrmann, Sebastian and others},
  journal={Journal of Machine Learning Research},
  volume={24},
  number={240},
  pages={1--113},
  year={2023}
}

@article{team2023gemini,
  title={Gemini: a family of highly capable multimodal models},
  author={Team, Gemini and Anil, Rohan and Borgeaud, Sebastian and Wu, Yonghui and Alayrac, Jean-Baptiste and Yu, Jiahui and Soricut, Radu and Schalkwyk, Johan and Dai, Andrew M and Hauth, Anja and others},
  journal={arXiv preprint arXiv:2312.11805},
  year={2023}
}

@article{touvron2023llama,
  title={Llama 2: Open foundation and fine-tuned chat models},
  author={Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
  journal={arXiv preprint arXiv:2307.09288},
  year={2023}
}

@article{jozefowicz2016exploring,
  title={Exploring the limits of language modeling},
  author={Jozefowicz, Rafal and Vinyals, Oriol and Schuster, Mike and Shazeer, Noam and Wu, Yonghui},
  journal={arXiv preprint arXiv:1602.02410},
  year={2016}
}

@article{jin2023growlength,
  title={Growlength: Accelerating llms pretraining by progressively growing training length},
  author={Jin, Hongye and Han, Xiaotian and Yang, Jingfeng and Jiang, Zhimeng and Chang, Chia-Yuan and Hu, Xia},
  journal={arXiv preprint arXiv:2310.00576},
  year={2023}
}

@article{bahdanau2014neural,
  title={Neural machine translation by jointly learning to align and translate},
  author={Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1409.0473},
  year={2014}
}

@article{anthropic2024claude,
  title={The claude 3 model family: Opus, sonnet, haiku},
  author={Anthropic, AI},
  journal={Claude-3 Model Card},
  volume={1},
  year={2024}
}

@article{achiam2023gpt,
  title={Gpt-4 technical report},
  author={Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and others},
  journal={arXiv preprint arXiv:2303.08774},
  year={2023}
}

@misc{naveed2023comprehensive,
  title={A comprehensive overview of large language models},
  author={Naveed, Humza and others},
  year={2023},
  note={arXiv preprint arXiv:2307.06435}
}

@misc{sadiq2023generative,
  title={Generative AI: Language models and multimodal foundation models},
  author={Sadiq, Shazia},
  year={2023}
}

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and others},
  journal={Advances in Neural Information Processing Systems},
  volume={30},
  year={2017}
}

@article{verma2023generative,
  title={Generative AI as a Tool for Enhancing Customer Relationship Management Automation and Personalization Techniques},
  author={Verma, Ramesh Kumar and Kumari, Nalini},
  journal={International Journal of Responsible Artificial Intelligence},
  volume={13},
  number={9},
  pages={1--8},
  year={2023}
}

@misc{villalobos2022machine,
  title={Machine learning model sizes and the parameter gap},
  author={Villalobos, Pablo and others},
  year={2022},
  note={arXiv preprint arXiv:2207.02852}
}

@misc{wang2018glue,
  title={GLUE: A multi-task benchmark and analysis platform for natural language understanding},
  author={Wang, Alex and others},
  year={2018},
  note={arXiv preprint arXiv:1804.07461}
}

@article{wang2019superglue,
  title={Superglue: A stickier benchmark for general-purpose language understanding systems},
  author={Wang, Alex and others},
  journal={Advances in Neural Information Processing Systems},
  volume={32},
  year={2019}
}

@misc{wei2022emergent,
  title={Emergent abilities of large language models},
  author={Wei, Jason and others},
  year={2022},
  note={arXiv preprint arXiv:2206.07682}
}

@misc{zellers2019hellaswag,
  title={Hellaswag: Can a machine really finish your sentence?},
  author={Zellers, Rowan and others},
  year={2019},
  note={arXiv preprint arXiv:1905.07830}
}

@article{es2023ragas,
  title={Ragas: Automated evaluation of retrieval augmented generation},
  author={Es, Shahul and James, Jithin and Espinosa-Anke, Luis and Schockaert, Steven},
  journal={arXiv preprint arXiv:2309.15217},
  year={2023}
}

@misc{hpa2024,
  title={HPA - High Performance Analytics},
  author={HPA},
  year={2024},
  note={\url{https://www.hpa.ai/}}
}

@misc{terranova2024,
  title={Terranova Software},
  author={{Terranova Software}},
  year={2024},
  note={\url{https://www.terranovasoftware.eu/en}}
}

@misc{langchain2024,
  title={LangChain},
  author={{LangChain}},
  year={2024},
  note={\url{https://www.langchain.com/}}
}

@article{dorri2018multi,
  title={Multi-agent systems: A survey},
  author={Dorri, Ali and Kanhere, Salil S and Jurdak, Raja},
  journal={Ieee Access},
  volume={6},
  pages={28573--28593},
  year={2018},
  publisher={IEEE}
}

@article{sanh2019distilbert,
  title={DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter},
  author={Sanh, Victor and Debut, Lysandre and Chaumond, Julien and Wolf, Thomas},
  journal={arXiv preprint arXiv:1910.01108},
  year={2019}
}

@article{abdullah2021multilingual,
  title={Multilingual Sentiment Analysis: A Systematic Literature Review.},
  author={Abdullah, Nur Atiqah Sia and Rusli, Nur Ida Aniza},
  journal={Pertanika Journal of Science \& Technology},
  volume={29},
  number={1},
  year={2021}
}

@article{schelter2017automatically,
  title={Automatically tracking metadata and provenance of machine learning experiments},
  author={Schelter, Sebastian and Boese, Joos-Hendrik and Kirschnick, Johannes and Klein, Thoralf and Seufert, Stephan},
  journal={},
  year={2017}
}

@article{zaharia2018accelerating,
  title={Accelerating the machine learning lifecycle with MLflow.},
  author={Zaharia, Matei and Chen, Andrew and Davidson, Aaron and Ghodsi, Ali and Hong, Sue Ann and Konwinski, Andy and Murching, Siddharth and Nykodym, Tomas and Ogilvie, Paul and Parkhe, Mani and others},
  journal={IEEE Data Eng. Bull.},
  volume={41},
  number={4},
  pages={39--45},
  year={2018}
}

@article{wu2022survey,
  title={A survey of human-in-the-loop for machine learning},
  author={Wu, Xingjiao and Xiao, Luwei and Sun, Yixuan and Zhang, Junhang and Ma, Tianlong and He, Liang},
  journal={Future Generation Computer Systems},
  volume={135},
  pages={364--381},
  year={2022},
  publisher={Elsevier}
}

@article{jiao2019tinybert,
  title={Tinybert: Distilling bert for natural language understanding},
  author={Jiao, Xiaoqi and Yin, Yichun and Shang, Lifeng and Jiang, Xin and Chen, Xiao and Li, Linlin and Wang, Fang and Liu, Qun},
  journal={arXiv preprint arXiv:1909.10351},
  year={2019}
}

@article{lan2019albert,
  title={Albert: A lite bert for self-supervised learning of language representations},
  author={Lan, Zhenzhong and Chen, Mingda and Goodman, Sebastian and Gimpel, Kevin and Sharma, Piyush and Soricut, Radu},
  journal={arXiv preprint arXiv:1909.11942},
  year={2019}
}

@article{ferrara2023fairness,
  title={Fairness and bias in artificial intelligence: A brief survey of sources, impacts, and mitigation strategies},
  author={Ferrara, Emilio},
  journal={Sci},
  volume={6},
  number={1},
  pages={3},
  year={2023},
  publisher={MDPI}
}

@misc{zhao2023survey,
  title={A survey of large language models},
  author={Zhao, Wayne Xin and others},
  year={2023},
  note={arXiv preprint arXiv:2303.18223}
}

@inproceedings{zheng2024judging,
  title={Judging LLM-as-a-judge with MT-Bench and chatbot arena},
  author={Zheng, Lianmin and others},
  booktitle={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}
